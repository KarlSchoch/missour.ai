{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2184e3e-fc92-486d-8e87-c868cea4a2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ID rough cost for doing semantic chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e69f6c83-aebc-44e0-917e-69f3ac903ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import tiktoken\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load OpenAI API key from .env file\n",
    "load_dotenv(\"../../.env\")\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b99131da-a7c0-4f38-a728-7520984b5210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in files (average size and largest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "645c08f9-f1dc-41ef-b4d6-ecddf91ad12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/dss-non-medicaid-pt2.txt\") as f:\n",
    "    largest_transcript = f.read()\n",
    "with open(\"../data/ltgov-dhss-p1.txt\") as f:\n",
    "    average_transcript = f.read() \n",
    "transcripts = {\n",
    "    'largest_transcript': largest_transcript,\n",
    "    'average_transcript': average_transcript,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "07812b56-7f9e-4c27-bd05-38e6c3393f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate text splitting cost for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "abc2f698-cda5-4393-aa2e-36598b2bf03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest_transcript (49384 tokens)\n",
      "* text-embedding-3-small estimated cost: 0.0009876800000000001\n",
      "* text-embedding-3-large estimated cost: 0.00641992\n",
      "average_transcript (17929 tokens)\n",
      "* text-embedding-3-small estimated cost: 0.00035858\n",
      "* text-embedding-3-large estimated cost: 0.00233077\n"
     ]
    }
   ],
   "source": [
    "embedding_cost = {\n",
    "    'text-embedding-3-small': .02/1000000,\n",
    "    'text-embedding-3-large': .13/1000000\n",
    "}\n",
    "for transcript in transcripts:\n",
    "    encoding = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    num_tokens = len(encoding.encode(transcripts[transcript]))\n",
    "    print(f\"{transcript} ({num_tokens} tokens)\")\n",
    "    for model in ['text-embedding-3-small', 'text-embedding-3-large']:\n",
    "        print(f\"* {model} estimated cost: {embedding_cost[model] * num_tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c7d2a21-f136-45cc-a646-ada3a40b44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Text Splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37b5d2f2-9293-4e5e-b2bb-95c2bbf65c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = SemanticChunker(OpenAIEmbeddings(openai_api_key=openai_api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "739d9a77-077d-4fd3-ac74-0261cb3373ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute chunking on longer transcript with largest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c5c51639-bae8-41c1-9a6e-e17bd59ae88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = text_splitter.create_documents([transcripts['largest_transcript']])\n",
    "# actual cost based on looking at the dashboard: <$0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20329f78-8dda-412c-98be-dcbfeab931e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translate chunks into dataframe and export csv for labeling to facilitate evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f85042bc-c3b7-4b0e-b86a-7b53430ccfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'chunk': []}\n",
    "\n",
    "for doc in docs:\n",
    "    data['chunk'].append(doc.page_content)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"../data/dss-non-medicaid-pt2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5be87f0-f79d-463b-ae19-2535ad630e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk files with the most references to IT spend according to Chat GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6ecdea-d706-4322-9820-924f0df3beac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_file(file_name:str):\n",
    "    \"\"\"\n",
    "    Takes a file name input and outputs a semantically chunked version of the file to the path with '-chunked.csv' appended to it\n",
    "\n",
    "    file_name: assumes that file is situated within the '../data/' directory.  See example below\n",
    "        - file_name: 'sa-dolir-solir'\n",
    "        - read path: '../data/sa-dolir-solir.txt'\n",
    "        - output path: '../data/sa-dolir-solir-chunked.csv'\n",
    "    \"\"\"\n",
    "    # Read in the file\n",
    "    with open(f\"../data/{file_name}.txt\") as f:\n",
    "        txt_file = f.read()\n",
    "    # Chunk text\n",
    "    docs = text_splitter.create_documents([txt_file])\n",
    "    # Transform into dataframe\n",
    "    data = {'chunk': []}\n",
    "    for doc in docs:\n",
    "        data['chunk'].append(doc.page_content)\n",
    "    df = pd.DataFrame(data)\n",
    "    # Output file to csv\n",
    "    output_file_path = f\"../data/{file_name}-chunked.csv\"\n",
    "    df.to_csv(output_file_path, index=False)\n",
    "    print(f\"Chunked file successfully output to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77e0a820-dbb9-440f-81b2-ca63eede09e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/sa-dolir.txt\") as f:\n",
    "    sa_dolir = f.read()\n",
    "docs = text_splitter.create_documents([sa_dolir])\n",
    "\n",
    "data = {'chunk': []}\n",
    "\n",
    "for doc in docs:\n",
    "    data['chunk'].append(doc.page_content)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"../data/sa-dolir-chunked.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b79d7f8-c352-4ee1-b698-6ddcde1de8e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content=\"So, this is the core budget. This funds everything except Medicaid fraud compliance and our safe kit. Absent any questions, I'll move to page 18. This is an NDI for our Public Protection and Criminal Appeals Division. Last year, we had 473 appeals. We filed 412 briefs, argued more than 100 cases in the Missouri Court of Appeals and the Missouri Supreme Court. As of January 1st, 2025, we've got 1,367 pending appeals. As this body knows, we handle 100% of felony appeals. Cops catch bad guys, prosecutors lock them up, and then we defend those convictions on appeal on behalf of the state. So the volume and workload has increased over time. We have special prosecutors that prosecute across the state of Missouri. They civilly commit sexually violent predators. Most of the cases that we were involved in at the trial court level are child sex cases, sexual assault cases, and murders, so we're putting the worst of the worst away. In 2024, we had cases in 28 different counties across the state, 130 counts, convicted five murders, and this is really about supporting some of our rural prosecutors, many of whom are still part-time. So it's a state resource that we deploy in the fight against violent crime. And leads have increased in those NDI. Moving to page 13, this is our Government Affairs and Litigation and Employment Law section. As this body recalls, we split out our employment law attorneys. Deep analysis revealed that 65% of our civil cases against the state are employment law actions against the state agencies. As those state agencies have grown, as our constitutional amendment has added a state agency, our liability has increased. Those cases have also become more complex in that there are more public interest groups filing more class actions lawsuits against the state today. And so as those operational needs change and grow because of the complexity of litigation grows and the number of cases grows, we need to meet those operational needs. I want to point out that in our civil defense section, a new mentorship program has allowed us to obtain seven complete defense verdicts in employment law actions against the state that saved the state north of $50 million. Additionally, we had 12 cases that we were able to get dismissed before they even got to trial, either a dispositive motion on the pleadings or summary judgment. Again, that's an enormous savings to the public fisc. We've got in governmental affairs right now, we typically run about 1300 cases per year. This year, we've got 1500 cases. So you've seen an increase in growth in those cases. We've also seen a 10 to 15% increase in our caseload in our litigation section. But one win that I want to point out there is our defense of the tort victims compensation fund, where we save the state $124 million by winning that case. Barring questions in our civil section. Moving to tab 16. This is our consumer protection division. As this body will recall, two years ago, I requested a nominal increase in our appropriations authority out of the merchandising practices revolving fund in order to fund six additional FTE that were already on the books in our consumer division that's ramped up our consumer protection work. I would point out also that the nature of consumer protection work has changed over time in the 1990s. What we typically saw were individual entities that were regionally isolated, that were defrauding Missouri consumers. Now we see more complex scams involving electronic means of communication. So again, you see a broader and more complex caseload within our consumer division. This body approved a nominal increase in approvals authority out of the NPRF. My first year in office, first year in office, we recovered 32 million in settlements and judgments on behalf of the state. You funded six additional FTE that were already on the books. Return on the investment this last year was recovered north of $400 million in settlements and judgments on behalf of the state. So proud of the work we're doing in our consumer tech protection division. This is not general revenue. Attorney General, when you give us those numbers, how long does it take the state probably in many cases to actually end up seeing those dollars and are we continuing to go to court and I'm not planning this for my own knowledge. Yeah. Wonder 400 million and it might take us 40 years to get the 400. It might take several years. I would say, you know, two to four years on average and that's an anecdotal estimation. But we we expedite specifically individual recovery for individual Missouri consumer victims. Missouri's first before. Thank you, sir. Barring any other questions, moving to tab 919. This is our labor division. So 10 years. Yeah, I believe it's actually it was in 2014. The General Assembly changed the statute as it relates to disability. So no longer a claim. Can you can claimants make claims for partial permanent disability? Now the only claim is full permanent disability based on seven enumerate statutory enumerated factors. When you change a statute, you wash away a sentiment of case law that interpreted the previous statute. Thus we're doing more appellate work because all of these cases using those seven enumerated factors get appealed and those are cases of first impression. So my labor division is doing more work on the appellate side as the courts interpret the new statute that's now been in place for about 10 years. So we're just trying to catch up with that. Barring any questions on our labor section, moving to tab 22. This is our solicitor office. This is the top appellate attorneys in the state that defend the statutes enacted by the Missouri General Assembly. Barring any questions on solicitor's office. Moving to page 2025. This is the governor's retention schedule for which we are supportive. We have seen and we've seen a symbiotic relationship between recruitment, retention and mentorship. Those have all helped improve our staffing levels. We were hovering at a 25% vacancy rate. When I took over, we've reduced that down to single digits feel like we're pretty close to fully staffed, even though that's a somewhat of a moving target. Retention is the next piece. When I took over, we had a lot of privates and a lot of lieutenants, not a lot of sergeants. In 2022, about 67 attorneys left the office. 2023, we stabilized about 44 attorneys left the office. I'm proud to say last year, we only had 20 attorneys leave the office. And again, that's a direct directly attributable to our ability to recruit talent, and thus manage a better caseload for our attorneys. And then our mentorship program that has resulted in those seven complete defense verdicts. It's an innovative first of its kind in the nation. We're providing better training and mentorship for our attorneys, which is causing them to stay longer and work harder. Thank you.\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/ded.txt\") as f:\n",
    "    ded = f.read()\n",
    "docs = text_splitter.create_documents([ded])\n",
    "\n",
    "data = {'chunk': []}\n",
    "\n",
    "for doc in docs:\n",
    "    data['chunk'].append(doc.page_content)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"../data/ded-chunked.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2409d6b2-4623-4b00-97d4-995f589d8022",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'chunk': []}\n",
    "\n",
    "for doc in docs:\n",
    "    data['chunk'].append(doc.page_content)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"../data/ded-chunked.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
